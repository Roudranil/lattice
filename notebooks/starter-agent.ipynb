{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44b53a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[2m10:36 AM\u001b[0m\u001b[37m\u001b[0m | \u001b[37m\u001b[2m__main__\u001b[0m\u001b[37m\u001b[0m | \u001b[37m\u001b[22mDEBUG    \u001b[0m | \u001b[37m\u001b[22msettings loaded as \n",
      "{\n",
      "  \"models\": {\n",
      "    \"hf\": {\n",
      "      \"chat\": \"EssentialAI/rnj-1-instruct:together\",\n",
      "      \"embedding_snowflake\": \"Snowflake/snowflake-arctic-embed-l-v2.0\",\n",
      "      \"embedding_specter\": \"allenai/specter2_base\",\n",
      "      \"encoder\": \"m3rg-iitd/matscibert\",\n",
      "      \"reranker\": \"Qwen/Qwen3-Reranker-0.6B\",\n",
      "      \"router\": \"openai/gpt-oss-20b:together\"\n",
      "    },\n",
      "    \"nebius\": {\n",
      "      \"reasoning\": \"zai-org/GLM-4.5-Air\",\n",
      "      \"tool_user\": \"Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
      "      \"chat\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-fast\",\n",
      "      \"embedding_baai_bge\": \"BAAI/bge-multilingual-gemma2\",\n",
      "      \"router\": \"openai/gpt-oss-20b\"\n",
      "    }\n",
      "  },\n",
      "  \"paths\": {\n",
      "    \"base_dir\": \"/home/rudy/code/lattice\",\n",
      "    \"data_dir\": \"/home/rudy/code/lattice/data\",\n",
      "    \"logs_dir\": \"/home/rudy/code/lattice/logs\",\n",
      "    \"agents_dir\": \"/home/rudy/code/lattice/agents\",\n",
      "    \"skills_dir\": \"/home/rudy/code/lattice/skills\",\n",
      "    \"tools_dir\": \"/home/rudy/code/lattice/tools\"\n",
      "  }\n",
      "}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "\n",
    "from src.settings import get_settings\n",
    "from src.utils.logger import create_logger\n",
    "\n",
    "settings = get_settings()\n",
    "logger = create_logger(path=settings.paths.logs_dir)\n",
    "logger.debug(f\"settings loaded as \\n{settings.model_dump_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "426acfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, List, Literal, Tuple, Annotated, Dict, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic.json_schema import SkipJsonSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompts.system import PRIMER_SYSTEM_PROMPT\n",
    "\n",
    "version = \"0.0.1-alpha\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296ad9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=settings.models.hf.router,\n",
    "    base_url=settings.env.HF_API_ENDPOINT,\n",
    "    api_key=settings.env.HF_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18ee69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchSubQuestion(BaseModel):\n",
    "    subquestion_text: str = Field(\n",
    "        description=\"A secondary, granular question that breaks down the main research question into actionable parts.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ResearchQuestion(BaseModel):\n",
    "    question_text: str = Field(\n",
    "        description=\"A high-level, focused research question (RQ) addressing a specific physical phenomenon or material property.\"\n",
    "    )\n",
    "    research_subquestions: Optional[List[ResearchSubQuestion]] = Field(\n",
    "        default=None,\n",
    "        description=\"A list of 2-3 granular sub-questions that help in answering the primary RQ.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ResearchPlan(BaseModel):\n",
    "    created_at: SkipJsonSchema[datetime] = Field(\n",
    "        default_factory=datetime.now,\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        description=\"A concise summary of the research goals. Max 4-5 sentences focusing on the 'what', 'why' and 'how'.\"\n",
    "    )\n",
    "    keywords: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=\"List of relevant scientific terms, experimental methods etc.\",\n",
    "    )\n",
    "    research_questions: List[ResearchQuestion] = Field(\n",
    "        description=\"A set of 3-5 primary research questions that form the backbone of the literature survey.\"\n",
    "    )\n",
    "    is_approved_by_user: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Set to True if the research_plan is complete and approved by the user explicitly. Always set to False if the research_plan is being created for the first time.\",\n",
    "    )\n",
    "\n",
    "    def to_markdown(self) -> str:\n",
    "        \"\"\"Converts the ResearchPlan object into a structured Markdown document.\"\"\"\n",
    "        timestamp = self.created_at.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        md = f\"# RESEARCH PLAN\\n {timestamp} by Lattice.\\n\"\n",
    "\n",
    "        # Summary Section\n",
    "        md += f\"## Summary\\n{self.summary}\\n\\n\"\n",
    "\n",
    "        # Keywords Section\n",
    "        if self.keywords:\n",
    "            md += \"## Keywords\\n\"\n",
    "            md += \", \".join([f\"`{k}`\" for k in self.keywords]) + \"\\n\\n\"\n",
    "\n",
    "        # Research Questions Section\n",
    "        md += \"## Research Questions\\n\"\n",
    "        for i, rq in enumerate(self.research_questions, 1):\n",
    "            md += f\"### RQ{i}: {rq.question_text}\\n\"\n",
    "\n",
    "            if rq.research_subquestions:\n",
    "                for j, sub in enumerate(rq.research_subquestions, 1):\n",
    "                    md += f\"  - **Sub-{j}**: {sub.subquestion_text}\\n\"\n",
    "            md += \"\\n\"\n",
    "        return md\n",
    "\n",
    "\n",
    "class PlannerResponse(BaseModel):\n",
    "    message: str = Field(\n",
    "        description=\"The response or message from the agent. The response or message MUST NOT be the research_plan.\"\n",
    "    )\n",
    "    research_plan: Optional[ResearchPlan] = Field(\n",
    "        \"This will contain ONLY the research_plan created by the agent. This is to be created when the agent is sure of the plan to be made and has asked all relevant clarifications to the user.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd78769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchPlanValidationRoute(BaseModel):\n",
    "    step: bool = Field(\n",
    "        description=\"Defines whether the research_plan exists or not.\",\n",
    "        default=False,\n",
    "    )\n",
    "    is_approved_by_user: bool = Field(\n",
    "        description=\"Defines whether the research_plan is approved by the user or not.\",\n",
    "        default=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    research_plan: Optional[ResearchPlan]\n",
    "    is_plan_approved_by_user: bool\n",
    "\n",
    "\n",
    "class RouterState(TypedDict):\n",
    "    deicision: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50742fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: AgentState) -> AgentState | List | Dict:\n",
    "    structured_llm = llm.with_structured_output(PlannerResponse, include_raw=True)\n",
    "    SYSTEM_PROMPT = PRIMER_SYSTEM_PROMPT.format(\n",
    "        date=datetime.datetime.now().strftime(\"%B %Y\"), version=\"0.0.1-alpha\"\n",
    "    )\n",
    "    response = structured_llm.invoke([SystemMessage(SYSTEM_PROMPT)] + state[\"messages\"])\n",
    "    raw_message = response[\"raw\"]\n",
    "    parsed_plan = response[\"parsed\"]\n",
    "    # return an update to the state\n",
    "    update = {\"messages\": raw_message, \"is_plan_approved_by_user\": False}\n",
    "    if parsed_plan and parsed_plan.research_plan:\n",
    "        update[\"research_plan\"] = research_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_node(state: RouterState) -> RouterState | List | Dict:\n",
    "    router = llm.with_structured_output(ResearchPlanValidationRoute)\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Decide if a research plan has been implemented based on conversation history.\"\n",
    "            )\n",
    "        ]\n",
    "        + state[\"messages\"]\n",
    "    )\n",
    "    return {\"decision\": decision}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
